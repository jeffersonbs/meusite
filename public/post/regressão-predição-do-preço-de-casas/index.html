<!DOCTYPE html>
<html lang="en">
  <head>
    
      <title>Regressão: predição do preço de casas :: Jefferson Barbosa — Portfólio</title>
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="description" content="Sobre os dados  Os dados utilizados aqui são provenientes da competição do Kaggle chamada de House prices: Advanced Regression Techniques. O conjunto de dados contêm 80 variáveis explicativas sobre algumas casas em Ames/Iowa e o objetivo é o de predizer o valor de venda dessas casas.
Nesta publicação eu mostrarei o passo a passo da minha análise, mostrado a forma como eu limpei os dados, o feature engineering e a modelagem."/>
<meta name="keywords" content=""/>
<meta name="robots" content="noodp"/>
<link rel="canonical" href="/post/regress%C3%A3o-predi%C3%A7%C3%A3o-do-pre%C3%A7o-de-casas/" />





<link rel="stylesheet" href="/assets/style.css">


<link rel="stylesheet" href="/style.css">


<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/img/apple-touch-icon-144-precomposed.png">
<link rel="shortcut icon" href="/img/favicon.png">


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Regressão: predição do preço de casas"/>
<meta name="twitter:description" content="Sobre os dados  Os dados utilizados aqui são provenientes da competição do Kaggle chamada de House prices: Advanced Regression Techniques. O conjunto de dados contêm 80 variáveis explicativas sobre algumas casas em Ames/Iowa e o objetivo é o de predizer o valor de venda dessas casas.
Nesta publicação eu mostrarei o passo a passo da minha análise, mostrado a forma como eu limpei os dados, o feature engineering e a modelagem."/>



<meta property="og:title" content="Regressão: predição do preço de casas" />
<meta property="og:description" content="Sobre os dados  Os dados utilizados aqui são provenientes da competição do Kaggle chamada de House prices: Advanced Regression Techniques. O conjunto de dados contêm 80 variáveis explicativas sobre algumas casas em Ames/Iowa e o objetivo é o de predizer o valor de venda dessas casas.
Nesta publicação eu mostrarei o passo a passo da minha análise, mostrado a forma como eu limpei os dados, o feature engineering e a modelagem." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/regress%C3%A3o-predi%C3%A7%C3%A3o-do-pre%C3%A7o-de-casas/" />
<meta property="article:published_time" content="2019-10-07T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-10-07T00:00:00+00:00" /><meta property="og:site_name" content="Jefferson Barbosa" />






  </head>
  <body class="">
    <div class="container">
      <header class="header">
  <span class="header__inner">
    <a href="/" class="logo" style="text-decoration: none;">
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367"/>
</svg>
</span>
    <span class="logo__text">Jefferson Barbosa</span>
    <span class="logo__cursor"></span>
  
</a>

    <span class="header__right">
      
        <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">Sobre</a></li>
        
      
      
    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">Sobre</a></li>
      
    
  </ul>
</nav>

        <span class="menu-trigger">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M0 0h24v24H0z" fill="none"/>
            <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
          </svg>
        </span>
      
      <span class="theme-toggle">
        <svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>

      </span>
    </span>
  </span>
</header>


      <div class="content">
        
  <div class="post">
    <h2 class="post-title"><a href="/post/regress%C3%A3o-predi%C3%A7%C3%A3o-do-pre%C3%A7o-de-casas/">Regressão: predição do preço de casas</a></h2>
    
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-149326010-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-149326010-1');
</script>
    <div class="post-meta">
      
        <span class="post-date">
            2019-10-07
        </span>
      
      <span class="post-author">— Escrito por Jefferson Barbosa</span>
      
        <span class="post-read-time">— 30 min de leitura</span>
      
    </div>

    
      <span class="post-tags">
        
          #<a href="/tags/regression/">regression</a>&nbsp;
        
          #<a href="/tags/random-forest/">random forest</a>&nbsp;
        
          #<a href="/tags/gradient-boosting/">gradient boosting</a>&nbsp;
        
          #<a href="/tags/support-vector-machines/">support vector machines</a>&nbsp;
        
      </span>
    

    

    <div class="post-content">
        



<h2>
Sobre os dados
</h2>
<p>Os dados utilizados aqui são provenientes da competição do <a href="kaggle.com">Kaggle</a> chamada de <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview">House prices: Advanced Regression Techniques</a>. O conjunto de dados contêm 80 variáveis explicativas sobre algumas casas em Ames/Iowa e o objetivo é o de predizer o valor de venda dessas casas.</p>
<p>Nesta publicação eu mostrarei o passo a passo da minha análise, mostrado a forma como eu limpei os dados, o feature engineering e a modelagem. Vou fazer três modelos e um ensemble, utilizando random forest, stochastic gradient boosting e support vector machines com kernel radial. Os pesos do ensemble serão estimados utilizando um modelo linear e compararei o desempenho dos modelos utilizando o RMSE.</p>
<h2>
Processamento dos dados
</h2>
<p>Nesta etapa eu utilizarei os seguintes pacotes:</p>
<pre class="r"><code>library(dplyr)
library(naniar)
library(corrplot)
library(ggplot2)
library(randomForest)</code></pre>
<p>O conjunto de dados conta com 1460 observações e 80 variáveis explicativas.</p>
<pre class="r"><code>str(dados)</code></pre>
<pre><code>## &#39;data.frame&#39;:    1460 obs. of  81 variables:
##  $ Id           : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ MSSubClass   : int  60 20 60 70 60 50 20 60 50 190 ...
##  $ MSZoning     : chr  &quot;RL&quot; &quot;RL&quot; &quot;RL&quot; &quot;RL&quot; ...
##  $ LotFrontage  : int  65 80 68 60 84 85 75 NA 51 50 ...
##  $ LotArea      : int  8450 9600 11250 9550 14260 14115 10084 10382 6120 7420 ...
##  $ Street       : chr  &quot;Pave&quot; &quot;Pave&quot; &quot;Pave&quot; &quot;Pave&quot; ...
##  $ Alley        : chr  NA NA NA NA ...
##  $ LotShape     : chr  &quot;Reg&quot; &quot;Reg&quot; &quot;IR1&quot; &quot;IR1&quot; ...
##  $ LandContour  : chr  &quot;Lvl&quot; &quot;Lvl&quot; &quot;Lvl&quot; &quot;Lvl&quot; ...
##  $ Utilities    : chr  &quot;AllPub&quot; &quot;AllPub&quot; &quot;AllPub&quot; &quot;AllPub&quot; ...
##  $ LotConfig    : chr  &quot;Inside&quot; &quot;FR2&quot; &quot;Inside&quot; &quot;Corner&quot; ...
##  $ LandSlope    : chr  &quot;Gtl&quot; &quot;Gtl&quot; &quot;Gtl&quot; &quot;Gtl&quot; ...
##  $ Neighborhood : chr  &quot;CollgCr&quot; &quot;Veenker&quot; &quot;CollgCr&quot; &quot;Crawfor&quot; ...
##  $ Condition1   : chr  &quot;Norm&quot; &quot;Feedr&quot; &quot;Norm&quot; &quot;Norm&quot; ...
##  $ Condition2   : chr  &quot;Norm&quot; &quot;Norm&quot; &quot;Norm&quot; &quot;Norm&quot; ...
##  $ BldgType     : chr  &quot;1Fam&quot; &quot;1Fam&quot; &quot;1Fam&quot; &quot;1Fam&quot; ...
##  $ HouseStyle   : chr  &quot;2Story&quot; &quot;1Story&quot; &quot;2Story&quot; &quot;2Story&quot; ...
##  $ OverallQual  : int  7 6 7 7 8 5 8 7 7 5 ...
##  $ OverallCond  : int  5 8 5 5 5 5 5 6 5 6 ...
##  $ YearBuilt    : int  2003 1976 2001 1915 2000 1993 2004 1973 1931 1939 ...
##  $ YearRemodAdd : int  2003 1976 2002 1970 2000 1995 2005 1973 1950 1950 ...
##  $ RoofStyle    : chr  &quot;Gable&quot; &quot;Gable&quot; &quot;Gable&quot; &quot;Gable&quot; ...
##  $ RoofMatl     : chr  &quot;CompShg&quot; &quot;CompShg&quot; &quot;CompShg&quot; &quot;CompShg&quot; ...
##  $ Exterior1st  : chr  &quot;VinylSd&quot; &quot;MetalSd&quot; &quot;VinylSd&quot; &quot;Wd Sdng&quot; ...
##  $ Exterior2nd  : chr  &quot;VinylSd&quot; &quot;MetalSd&quot; &quot;VinylSd&quot; &quot;Wd Shng&quot; ...
##  $ MasVnrType   : chr  &quot;BrkFace&quot; &quot;None&quot; &quot;BrkFace&quot; &quot;None&quot; ...
##  $ MasVnrArea   : int  196 0 162 0 350 0 186 240 0 0 ...
##  $ ExterQual    : chr  &quot;Gd&quot; &quot;TA&quot; &quot;Gd&quot; &quot;TA&quot; ...
##  $ ExterCond    : chr  &quot;TA&quot; &quot;TA&quot; &quot;TA&quot; &quot;TA&quot; ...
##  $ Foundation   : chr  &quot;PConc&quot; &quot;CBlock&quot; &quot;PConc&quot; &quot;BrkTil&quot; ...
##  $ BsmtQual     : chr  &quot;Gd&quot; &quot;Gd&quot; &quot;Gd&quot; &quot;TA&quot; ...
##  $ BsmtCond     : chr  &quot;TA&quot; &quot;TA&quot; &quot;TA&quot; &quot;Gd&quot; ...
##  $ BsmtExposure : chr  &quot;No&quot; &quot;Gd&quot; &quot;Mn&quot; &quot;No&quot; ...
##  $ BsmtFinType1 : chr  &quot;GLQ&quot; &quot;ALQ&quot; &quot;GLQ&quot; &quot;ALQ&quot; ...
##  $ BsmtFinSF1   : int  706 978 486 216 655 732 1369 859 0 851 ...
##  $ BsmtFinType2 : chr  &quot;Unf&quot; &quot;Unf&quot; &quot;Unf&quot; &quot;Unf&quot; ...
##  $ BsmtFinSF2   : int  0 0 0 0 0 0 0 32 0 0 ...
##  $ BsmtUnfSF    : int  150 284 434 540 490 64 317 216 952 140 ...
##  $ TotalBsmtSF  : int  856 1262 920 756 1145 796 1686 1107 952 991 ...
##  $ Heating      : chr  &quot;GasA&quot; &quot;GasA&quot; &quot;GasA&quot; &quot;GasA&quot; ...
##  $ HeatingQC    : chr  &quot;Ex&quot; &quot;Ex&quot; &quot;Ex&quot; &quot;Gd&quot; ...
##  $ CentralAir   : chr  &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; ...
##  $ Electrical   : chr  &quot;SBrkr&quot; &quot;SBrkr&quot; &quot;SBrkr&quot; &quot;SBrkr&quot; ...
##  $ X1stFlrSF    : int  856 1262 920 961 1145 796 1694 1107 1022 1077 ...
##  $ X2ndFlrSF    : int  854 0 866 756 1053 566 0 983 752 0 ...
##  $ LowQualFinSF : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ GrLivArea    : int  1710 1262 1786 1717 2198 1362 1694 2090 1774 1077 ...
##  $ BsmtFullBath : int  1 0 1 1 1 1 1 1 0 1 ...
##  $ BsmtHalfBath : int  0 1 0 0 0 0 0 0 0 0 ...
##  $ FullBath     : int  2 2 2 1 2 1 2 2 2 1 ...
##  $ HalfBath     : int  1 0 1 0 1 1 0 1 0 0 ...
##  $ BedroomAbvGr : int  3 3 3 3 4 1 3 3 2 2 ...
##  $ KitchenAbvGr : int  1 1 1 1 1 1 1 1 2 2 ...
##  $ KitchenQual  : chr  &quot;Gd&quot; &quot;TA&quot; &quot;Gd&quot; &quot;Gd&quot; ...
##  $ TotRmsAbvGrd : int  8 6 6 7 9 5 7 7 8 5 ...
##  $ Functional   : chr  &quot;Typ&quot; &quot;Typ&quot; &quot;Typ&quot; &quot;Typ&quot; ...
##  $ Fireplaces   : int  0 1 1 1 1 0 1 2 2 2 ...
##  $ FireplaceQu  : chr  NA &quot;TA&quot; &quot;TA&quot; &quot;Gd&quot; ...
##  $ GarageType   : chr  &quot;Attchd&quot; &quot;Attchd&quot; &quot;Attchd&quot; &quot;Detchd&quot; ...
##  $ GarageYrBlt  : int  2003 1976 2001 1998 2000 1993 2004 1973 1931 1939 ...
##  $ GarageFinish : chr  &quot;RFn&quot; &quot;RFn&quot; &quot;RFn&quot; &quot;Unf&quot; ...
##  $ GarageCars   : int  2 2 2 3 3 2 2 2 2 1 ...
##  $ GarageArea   : int  548 460 608 642 836 480 636 484 468 205 ...
##  $ GarageQual   : chr  &quot;TA&quot; &quot;TA&quot; &quot;TA&quot; &quot;TA&quot; ...
##  $ GarageCond   : chr  &quot;TA&quot; &quot;TA&quot; &quot;TA&quot; &quot;TA&quot; ...
##  $ PavedDrive   : chr  &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; ...
##  $ WoodDeckSF   : int  0 298 0 0 192 40 255 235 90 0 ...
##  $ OpenPorchSF  : int  61 0 42 35 84 30 57 204 0 4 ...
##  $ EnclosedPorch: int  0 0 0 272 0 0 0 228 205 0 ...
##  $ X3SsnPorch   : int  0 0 0 0 0 320 0 0 0 0 ...
##  $ ScreenPorch  : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ PoolArea     : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ PoolQC       : chr  NA NA NA NA ...
##  $ Fence        : chr  NA NA NA NA ...
##  $ MiscFeature  : chr  NA NA NA NA ...
##  $ MiscVal      : int  0 0 0 0 0 700 0 350 0 0 ...
##  $ MoSold       : int  2 5 9 2 12 10 8 11 4 1 ...
##  $ YrSold       : int  2008 2007 2008 2006 2008 2009 2007 2009 2008 2008 ...
##  $ SaleType     : chr  &quot;WD&quot; &quot;WD&quot; &quot;WD&quot; &quot;WD&quot; ...
##  $ SaleCondition: chr  &quot;Normal&quot; &quot;Normal&quot; &quot;Normal&quot; &quot;Abnorml&quot; ...
##  $ SalePrice    : int  208500 181500 223500 140000 250000 143000 307000 200000 129900 118000 ...</code></pre>
<p>Olhando a descrição dos dados, vi que em algumas variáveis foi atribuído NA quando o elemento não estava presente. Irei alterar para uma string, tornando assim as observações úteis para a análise.</p>
<pre class="r"><code>#Alley
dados$Alley[which(is.na(dados$Alley))] = &quot;Naa&quot;

#Basement
dados$BsmtQual[which(is.na(dados$BsmtQual))] = &quot;Nb&quot;
dados$BsmtCond[which(is.na(dados$BsmtCond))] = &quot;Nb&quot;
dados$BsmtExposure[which(is.na(dados$BsmtExposure))] = &quot;Nb&quot;
dados$BsmtFinType1[which(is.na(dados$BsmtFinType1))] = &quot;Nb&quot;
dados$BsmtFinType2[which(is.na(dados$BsmtFinType2))] = &quot;Nb&quot;

#Fireplace
dados$FireplaceQu[which(is.na(dados$FireplaceQu))] = &quot;Nf&quot;

#Garage
dados$GarageType[which(is.na(dados$GarageType))] = &quot;Ng&quot;
dados$GarageFinish[which(is.na(dados$GarageFinish))] = &quot;Ng&quot;
dados$GarageQual[which(is.na(dados$GarageQual))] = &quot;Ng&quot;
dados$GarageCond[which(is.na(dados$GarageCond))] = &quot;Ng&quot;

#Pool
dados$PoolQC[which(is.na(dados$PoolQC))] = &quot;Np&quot;

#Fence
dados$Fence[which(is.na(dados$Fence))] = &quot;Nf&quot;</code></pre>
<p>O mesmo foi feito com a variável que indica o ano de construção da garagem, então nestes casos irei atribuir 0.</p>
<pre class="r"><code>dados$GarageYrBlt[which(is.na(dados$GarageYrBlt))] = 0</code></pre>
<p>As variáveis que indicam a identificação da casa, o mês, ano, tipo e condições da venda não serão uteis para a análise, então irei removê-las.</p>
<pre class="r"><code>dados &lt;- dados %&gt;%
  select(-c(MoSold, YrSold, SaleType, SaleCondition, Id))</code></pre>
<p>E em algumas variáveis categóricas o nível foi descrito por um número, portanto será necessário converter para uma string.</p>
<pre class="r"><code>dados$MSSubClass = as.character(dados$MSSubClass)
dados$OverallQual = as.character(dados$OverallQual)
dados$OverallCond = as.character(dados$OverallCond)</code></pre>
<p>O gráfico abaixo mostra a porcentagem de NA’s em cada variável.</p>
<pre class="r"><code>gg_miss_var(dados, show_pct = T) +
  labs(x = &quot;Variáveis&quot;,y = &quot;Porcentagem de NA&quot;)</code></pre>
<p><img src="/post/2019-09-30-regress%C3%A3o-prevendo-o-pre%C3%A7o-de-casas_files/figure-html/unnamed-chunk-8-1.png" width="672" />
Irei remover a variável com mais de 20% de NA’s</p>
<pre class="r"><code>dados &lt;- dados %&gt;%
  select(-c(MiscFeature))</code></pre>
<p>e vou imputar a categoria mais frequente ou a mediana das observações, caso seja numérica, nas demais variáveis.</p>
<pre class="r"><code>#Convertendo as strings em fatores 
#Encontra as colunas que possuem string
ind &lt;- sapply(dados, is.character)
#Converte para fator
dados[ind] &lt;- lapply(dados[ind], factor)

#Imputando dados- para a categoria mais frequente ou para a mediana caso numerico
dados &lt;- na.roughfix(dados)</code></pre>
<p>E agora não há mais NA’s nos dados.</p>
<pre class="r"><code>gg_miss_var(dados, show_pct = T) +
  labs(x = &quot;Variáveis&quot;,y = &quot;Porcentagem de NA&quot;)</code></pre>
<img src="/post/2019-09-30-regress%C3%A3o-prevendo-o-pre%C3%A7o-de-casas_files/figure-html/unnamed-chunk-11-1.png" width="672" />
<h3>
Feature engineering
</h3>
<p>Aqui irei criar algumas novas variáveis e aplicar transformações onde necessário.</p>
<p>Para começar irei criar uma variável com o total de banheiros na casa</p>
<pre class="r"><code>dados &lt;- dados %&gt;%
  mutate(bathrooms = BsmtFullBath + 0.5*BsmtHalfBath + FullBath + 0.5*HalfBath) %&gt;%
  select(-c(BsmtFullBath, BsmtHalfBath, FullBath, HalfBath))</code></pre>
<p>e uma para a área total da casa.</p>
<pre class="r"><code>dados &lt;- dados %&gt;%
  mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF) %&gt;%
  select(-c(X1stFlrSF, X2ndFlrSF, TotalBsmtSF))</code></pre>
<p>A distribuição da variável resposta SalePrice é bastante assimétrica à direita</p>
<pre class="r"><code>ggplot(dados, aes(x = SalePrice)) + 
  geom_density() + theme_bw()</code></pre>
<p><img src="/post/2019-09-30-regress%C3%A3o-prevendo-o-pre%C3%A7o-de-casas_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>e comparando os quantis da distribuição de SalePrice com os quantis teóricos da distribuição normal, podemos ver que a variável foge bastante da hipótese de normalidade.</p>
<pre class="r"><code>ggplot(dados, aes(sample = SalePrice)) +
  geom_qq() + geom_qq_line() + theme_bw()</code></pre>
<p><img src="/post/2019-09-30-regress%C3%A3o-prevendo-o-pre%C3%A7o-de-casas_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>O logaritmo natural parece amenizar bastante o problema da assimetria.</p>
<pre class="r"><code>ggplot(dados, aes(x = log(SalePrice))) + 
  geom_density() + theme_bw()</code></pre>
<p><img src="/post/2019-09-30-regress%C3%A3o-prevendo-o-pre%C3%A7o-de-casas_files/figure-html/unnamed-chunk-16-1.png" width="672" />
Mas apesar de estar bem mais próxima de uma gaussiana, a variável ainda não se adéqua a distribuição normal, o que não é um problema para os métodos que irei utilizar.</p>
<pre class="r"><code>ggplot(dados, aes(sample = log(SalePrice))) + 
  geom_qq() + geom_qq_line() + theme_bw()</code></pre>
<p><img src="/post/2019-09-30-regress%C3%A3o-prevendo-o-pre%C3%A7o-de-casas_files/figure-html/unnamed-chunk-17-1.png" width="672" />
Portanto, manterei a transformação.</p>
<pre class="r"><code>dados &lt;- dados %&gt;%
  mutate(SalePrice = log(SalePrice))</code></pre>
<p>Abaixo é mostrado o coeficiente de assimetria para as demais variáveis numéricas. Algumas delas possuem assimetrias bastante significativas e será necessário aplicar uma transformação para corrigir isto.</p>
<pre class="r"><code>psych::skew(select_if(dados, is.numeric))</code></pre>
<pre><code>##  [1]  2.4041986 12.1826150 -0.6122012 -0.5025278  2.6721170  1.6820413
##  [7]  4.2465214  0.9183784  8.9928333  1.3637536  0.2113551  4.4791783
## [13]  0.6749517  0.6482311 -3.8615534 -0.3418454  0.1796113  1.5382100
## [19]  2.3594857  3.0835258 10.2831784  4.1137473 14.7979183 24.4265224
## [25]  0.1210859  0.2641325  1.7730505</code></pre>
<p>Como essas variáveis possuem zeros ou valores negativos, não será possível aplicar o logaritmo natural como foi feito com SalePrice, então aplicarei a raiz cúbica de toda variável com assimetria menor que -0.75 ou maior que 0.75.</p>
<pre class="r"><code>ind &lt;- sapply(dados, is.numeric)
for(i in 1:ncol(dados)){
  if(ind[i]){
    if(psych::skew(dados[,i])&gt;0.75 | psych::skew(dados[,i]) &lt; -0.75){
      dados[,i] = (dados[,i])^(1/3)
    }
  }
}</code></pre>
<p>O que produziu bons resultados para a maioria dos casos.</p>
<pre class="r"><code>psych::skew(select_if(dados, is.numeric))</code></pre>
<pre><code>##  [1] -0.08708814  2.24511025 -0.61220121 -0.50252776  0.70529915
##  [6] -0.35450003  2.72602352 -1.00183220  7.70532558  0.38241571
## [11]  0.21135511  0.34503426  0.67495173  0.64823107 -3.87770277
## [16] -0.34184538  0.17961125  0.27269050  0.17711218  2.19119203
## [21]  7.90825015  3.21481019 14.37182501  6.45735540  0.12108586
## [26]  0.26413255  0.23135118</code></pre>
<p>Por fim ficamos com 69 variáveis e as mesmas 1460 observações.</p>
<pre class="r"><code>dim(dados)</code></pre>
<pre><code>## [1] 1460   70</code></pre>
<h2>
Modelagem
</h2>
<p>Para começar vou dividir os dados em um conjunto de treino, que corresponderá a 80% das observações, e um conjunto de validação com o restante e carregar os pacotes necessários nesta etapa.</p>
<pre class="r"><code>set.seed(576)
index = sample(1:dim(dados)[1], 1168, replace = F)
treino = dados[index,]
validacao = dados[-index,]</code></pre>
<pre class="r"><code>library(caret)
library(randomForest)
library(doMC)
library(gbm)
library(kernlab)</code></pre>
<p>Definindo o número de threads a ser utilizado.</p>
<pre class="r"><code>registerDoMC(cores = 4)</code></pre>
Em todos os casos será utilizada a validação cruzada repetida para definir a melhor combinação de hiperparâmetros. Por limitações de poder computacional nem sempre foi possível encontrar a combinação ótima dos hiperpârametros.
<h3>
Random Forest
</h3>
<p>Para começar irei fazer o tuning do parâmetro mtry do random forest, que representa o número de variáveis que participarão do sorteio na divisão de cada nó nas árvores.</p>
<pre class="r"><code>control &lt;- trainControl(method = &quot;repeatedcv&quot;, 
                      number = 10,
                      repeats = 3)

tunegrid &lt;- expand.grid(.mtry=seq(1,69,1))

model.rf.mtry = train(SalePrice~., data = treino, method = &quot;rf&quot;, tuneGrid=tunegrid, metric=&quot;RMSE&quot;, trControl=control)
model.rf.mtry</code></pre>
<pre><code>## Random Forest 
## 
## 1168 samples
##   69 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 1052, 1051, 1051, 1050, 1051, 1051, ... 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE       Rsquared   MAE       
##    1    0.3071331  0.7810402  0.22607670
##    2    0.2254758  0.8143281  0.15783921
##    3    0.1936032  0.8364856  0.13249279
##    4    0.1776564  0.8510921  0.12025337
##    5    0.1681296  0.8607840  0.11323940
##    6    0.1627488  0.8660263  0.10935524
##    7    0.1589866  0.8691845  0.10647024
##    8    0.1557965  0.8725056  0.10420378
##    9    0.1532778  0.8753523  0.10258097
##   10    0.1518917  0.8763699  0.10152837
##   11    0.1501778  0.8782940  0.10051130
##   12    0.1494971  0.8784993  0.10016042
##   13    0.1480806  0.8800844  0.09900142
##   14    0.1476059  0.8802354  0.09867569
##   15    0.1465570  0.8812828  0.09809760
##   16    0.1459670  0.8818427  0.09761319
##   17    0.1450209  0.8827507  0.09696197
##   18    0.1448914  0.8828137  0.09707160
##   19    0.1443649  0.8832091  0.09661135
##   20    0.1435585  0.8842482  0.09592553
##   21    0.1437793  0.8834199  0.09623241
##   22    0.1431377  0.8838096  0.09572380
##   23    0.1425945  0.8845429  0.09538230
##   24    0.1425120  0.8847815  0.09551756
##   25    0.1421370  0.8851159  0.09536504
##   26    0.1422302  0.8846188  0.09531927
##   27    0.1415267  0.8856568  0.09494621
##   28    0.1415552  0.8852868  0.09478804
##   29    0.1412062  0.8856048  0.09469074
##   30    0.1410151  0.8859585  0.09450404
##   31    0.1410779  0.8855805  0.09470133
##   32    0.1409702  0.8854917  0.09438240
##   33    0.1414394  0.8845036  0.09451632
##   34    0.1407760  0.8855469  0.09448911
##   35    0.1405608  0.8856942  0.09432777
##   36    0.1401718  0.8862803  0.09419182
##   37    0.1406369  0.8852311  0.09425733
##   38    0.1406003  0.8851706  0.09423406
##   39    0.1401908  0.8856175  0.09409481
##   40    0.1402438  0.8854223  0.09413707
##   41    0.1401805  0.8855006  0.09398179
##   42    0.1401042  0.8855886  0.09413101
##   43    0.1402814  0.8850709  0.09417576
##   44    0.1404239  0.8849399  0.09444120
##   45    0.1395261  0.8862789  0.09381134
##   46    0.1397598  0.8857732  0.09388258
##   47    0.1397561  0.8856509  0.09394146
##   48    0.1397801  0.8855166  0.09393174
##   49    0.1394734  0.8859182  0.09375560
##   50    0.1399445  0.8849246  0.09416178
##   51    0.1394826  0.8858094  0.09398916
##   52    0.1402274  0.8844410  0.09420846
##   53    0.1397968  0.8852276  0.09417695
##   54    0.1393756  0.8856854  0.09374741
##   55    0.1397599  0.8848938  0.09407772
##   56    0.1395496  0.8853540  0.09412655
##   57    0.1395353  0.8853507  0.09399841
##   58    0.1395029  0.8851668  0.09377249
##   59    0.1393193  0.8853031  0.09380104
##   60    0.1394579  0.8852845  0.09402501
##   61    0.1393637  0.8852169  0.09376707
##   62    0.1396414  0.8847520  0.09406628
##   63    0.1395461  0.8847076  0.09405419
##   64    0.1397357  0.8844860  0.09415471
##   65    0.1396264  0.8846549  0.09398381
##   66    0.1395992  0.8846408  0.09412881
##   67    0.1395331  0.8846109  0.09396411
##   68    0.1392987  0.8848799  0.09394804
##   69    0.1394037  0.8847799  0.09394884
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 68.</code></pre>
<p>Então o melhor valor de mtry é 68. Por fim farei uma busca pelo valor ótimo do número de árvores no modelo fixando o mtry como a raiz quadrada de 69, o número de variáveis.</p>
<pre class="r"><code>tunegrid &lt;- expand.grid(.mtry = sqrt(ncol(treino)))
modellist &lt;- list()

for (ntree in c(100,250,500,1000,1500,2000)){
  set.seed(250)
  fit &lt;- train(SalePrice~.,
               data = treino,
               method = &quot;rf&quot;,
               metric = &quot;RMSE&quot;,
               tuneGrid = tunegrid,
               trControl = control,
               ntree = ntree)
  key &lt;- toString(ntree)
  modellist[[key]] &lt;- fit
}
results &lt;- resamples(modellist)
dotplot(results)</code></pre>
<p><img src="/post/2019-09-30-regress%C3%A3o-prevendo-o-pre%C3%A7o-de-casas_files/figure-html/unnamed-chunk-29-1.png" width="672" />
O acréscimo no número de árvores não trouxe melhorias para o modelo, então pelo critério da parcimônia optarei por utilizar 100 árvores. Agora só resta ajustar o modelo final e calcular o RMSE.</p>
<pre class="r"><code>model.rf.final = randomForest(SalePrice~., data = treino, mtry = 68, ntree = 100)
rmse.rf.final = RMSE(exp(predict(model.rf.final,validacao)), exp(validacao$SalePrice))
rmse.rf.final</code></pre>
<pre><code>## [1] 29332.11</code></pre>
<h3>
Support Vector Machine
</h3>
<p>No support vector machine eu usarei o kernel radial e terei dois principais parâmetros para tunar.</p>
<pre class="r"><code>control &lt;- trainControl(method = &#39;repeatedcv&#39;,
                        number = 10,
                        repeats = 3)
svmRadialGrid &lt;- expand.grid(C = seq(1, 5, by=1),
                             sigma = seq(0.01, 0.2, by=0.01))

model.SvmRadial &lt;- train(SalePrice~., data = treino, 
                     method = &quot;svmRadial&quot;, 
                     trControl = control, 
                     metric = &quot;RMSE&quot;, tuneGrid = svmRadialGrid)
model.SvmRadial</code></pre>
<pre><code>## Support Vector Machines with Radial Basis Function Kernel 
## 
## 1168 samples
##   69 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 1052, 1052, 1052, 1051, 1052, 1051, ... 
## Resampling results across tuning parameters:
## 
##   C  sigma  RMSE       Rsquared    MAE      
##   1  0.01   0.3063900  0.47475452  0.2205563
##   1  0.02   0.3538665  0.33310385  0.2648665
##   1  0.03   0.3757942  0.23879533  0.2870956
##   1  0.04   0.3865575  0.17141460  0.2977579
##   1  0.05   0.3922794  0.12518648  0.3031724
##   1  0.06   0.3955770  0.09366059  0.3062704
##   1  0.07   0.3976214  0.07199871  0.3081836
##   1  0.08   0.3989676  0.05698089  0.3094441
##   1  0.09   0.3998994  0.04641032  0.3103160
##   1  0.10   0.4005724  0.03882228  0.3109473
##   1  0.11   0.4010767  0.03328378  0.3114225
##   1  0.12   0.4014656  0.02913234  0.3117916
##   1  0.13   0.4017727  0.02593969  0.3120841
##   1  0.14   0.4020204  0.02343000  0.3123216
##   1  0.15   0.4022233  0.02141950  0.3125183
##   1  0.16   0.4023918  0.01976339  0.3126820
##   1  0.17   0.4025334  0.01836626  0.3128207
##   1  0.18   0.4026535  0.01716876  0.3129394
##   1  0.19   0.4027562  0.01611473  0.3130419
##   1  0.20   0.4028446  0.01517701  0.3131310
##   2  0.01   0.3063687  0.47209227  0.2206516
##   2  0.02   0.3536250  0.33092094  0.2647102
##   2  0.03   0.3755922  0.23865845  0.2869160
##   2  0.04   0.3864254  0.17226298  0.2976609
##   2  0.05   0.3921974  0.12608664  0.3031157
##   2  0.06   0.3955256  0.09437363  0.3062350
##   2  0.07   0.3975873  0.07248128  0.3081579
##   2  0.08   0.3989452  0.05730797  0.3094250
##   2  0.09   0.3998844  0.04663286  0.3103023
##   2  0.10   0.4005616  0.03896073  0.3109344
##   2  0.11   0.4010687  0.03338352  0.3114116
##   2  0.12   0.4014593  0.02919614  0.3117815
##   2  0.13   0.4017675  0.02598511  0.3120745
##   2  0.14   0.4020158  0.02346816  0.3123125
##   2  0.15   0.4022193  0.02144219  0.3125089
##   2  0.16   0.4023882  0.01978318  0.3126734
##   2  0.17   0.4025300  0.01838051  0.3128121
##   2  0.18   0.4026501  0.01717281  0.3129304
##   2  0.19   0.4027529  0.01612068  0.3130334
##   2  0.20   0.4028412  0.01518229  0.3131225
##   3  0.01   0.3063687  0.47209227  0.2206516
##   3  0.02   0.3536250  0.33092094  0.2647102
##   3  0.03   0.3755922  0.23865845  0.2869160
##   3  0.04   0.3864254  0.17226298  0.2976609
##   3  0.05   0.3921974  0.12608664  0.3031157
##   3  0.06   0.3955256  0.09437363  0.3062350
##   3  0.07   0.3975873  0.07248128  0.3081579
##   3  0.08   0.3989452  0.05730797  0.3094250
##   3  0.09   0.3998844  0.04663286  0.3103023
##   3  0.10   0.4005616  0.03896073  0.3109344
##   3  0.11   0.4010687  0.03338352  0.3114116
##   3  0.12   0.4014593  0.02919614  0.3117815
##   3  0.13   0.4017675  0.02598511  0.3120745
##   3  0.14   0.4020158  0.02346816  0.3123125
##   3  0.15   0.4022193  0.02144219  0.3125089
##   3  0.16   0.4023882  0.01978318  0.3126734
##   3  0.17   0.4025300  0.01838051  0.3128121
##   3  0.18   0.4026501  0.01717281  0.3129304
##   3  0.19   0.4027529  0.01612068  0.3130334
##   3  0.20   0.4028412  0.01518229  0.3131225
##   4  0.01   0.3063687  0.47209227  0.2206516
##   4  0.02   0.3536250  0.33092094  0.2647102
##   4  0.03   0.3755922  0.23865845  0.2869160
##   4  0.04   0.3864254  0.17226298  0.2976609
##   4  0.05   0.3921974  0.12608664  0.3031157
##   4  0.06   0.3955256  0.09437363  0.3062350
##   4  0.07   0.3975873  0.07248128  0.3081579
##   4  0.08   0.3989452  0.05730797  0.3094250
##   4  0.09   0.3998844  0.04663286  0.3103023
##   4  0.10   0.4005616  0.03896073  0.3109344
##   4  0.11   0.4010687  0.03338352  0.3114116
##   4  0.12   0.4014593  0.02919614  0.3117815
##   4  0.13   0.4017675  0.02598511  0.3120745
##   4  0.14   0.4020158  0.02346816  0.3123125
##   4  0.15   0.4022193  0.02144219  0.3125089
##   4  0.16   0.4023882  0.01978318  0.3126734
##   4  0.17   0.4025300  0.01838051  0.3128121
##   4  0.18   0.4026501  0.01717281  0.3129304
##   4  0.19   0.4027529  0.01612068  0.3130334
##   4  0.20   0.4028412  0.01518229  0.3131225
##   5  0.01   0.3063687  0.47209227  0.2206516
##   5  0.02   0.3536250  0.33092094  0.2647102
##   5  0.03   0.3755922  0.23865845  0.2869160
##   5  0.04   0.3864254  0.17226298  0.2976609
##   5  0.05   0.3921974  0.12608664  0.3031157
##   5  0.06   0.3955256  0.09437363  0.3062350
##   5  0.07   0.3975873  0.07248128  0.3081579
##   5  0.08   0.3989452  0.05730797  0.3094250
##   5  0.09   0.3998844  0.04663286  0.3103023
##   5  0.10   0.4005616  0.03896073  0.3109344
##   5  0.11   0.4010687  0.03338352  0.3114116
##   5  0.12   0.4014593  0.02919614  0.3117815
##   5  0.13   0.4017675  0.02598511  0.3120745
##   5  0.14   0.4020158  0.02346816  0.3123125
##   5  0.15   0.4022193  0.02144219  0.3125089
##   5  0.16   0.4023882  0.01978318  0.3126734
##   5  0.17   0.4025300  0.01838051  0.3128121
##   5  0.18   0.4026501  0.01717281  0.3129304
##   5  0.19   0.4027529  0.01612068  0.3130334
##   5  0.20   0.4028412  0.01518229  0.3131225
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were sigma = 0.01 and C = 2.</code></pre>
<p>Agora ajustando o modelo final para C=2 e sigma=0.01 e calculando o RMSE.</p>
<pre class="r"><code>svm.final &lt;- ksvm(SalePrice~., data = treino, C = 2, sigma = 0.01)
rmse.svm.final &lt;- RMSE(exp(predict(svm.final, validacao)), exp(validacao$SalePrice))
rmse.svm.final &lt;- RMSE(exp(predict(model.svm.final, validacao)),exp(validacao$SalePrice))
rmse.svm.final</code></pre>
<pre><code>## [1] 32341.42</code></pre>
<h3>
Stochastic Gradient Boosting
</h3>
<p>No stochastic gradient boosting temos 4 hiperparâmetros para tunar.</p>
<pre class="r"><code>gbmGrid &lt;-  gbmGrid &lt;-  expand.grid(interaction.depth = c(4, 8, 12), 
                                    n.trees = (1:15)*25, 
                                    shrinkage = c(.01, .1, .3),
                                    n.minobsinnode = c(8, 12))

model.gbm = train(SalePrice~., data = treino, method = &quot;gbm&quot;,
              distribution = &quot;gaussian&quot;,
              trControl = control,
              tuneGrid = gbmGrid,
              verbose = F)
model.gbm</code></pre>
<pre><code>## Stochastic Gradient Boosting 
## 
## 1168 samples
##   69 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 1052, 1051, 1051, 1050, 1052, 1052, ... 
## Resampling results across tuning parameters:
## 
##   shrinkage  interaction.depth  n.minobsinnode  n.trees  RMSE     
##   0.01        4                  8               25      0.3452519
##   0.01        4                  8               50      0.3002047
##   0.01        4                  8               75      0.2652022
##   0.01        4                  8              100      0.2383534
##   0.01        4                  8              125      0.2174696
##   0.01        4                  8              150      0.2014443
##   0.01        4                  8              175      0.1889937
##   0.01        4                  8              200      0.1794419
##   0.01        4                  8              225      0.1718025
##   0.01        4                  8              250      0.1657776
##   0.01        4                  8              275      0.1609140
##   0.01        4                  8              300      0.1567877
##   0.01        4                  8              325      0.1532826
##   0.01        4                  8              350      0.1504682
##   0.01        4                  8              375      0.1480495
##   0.01        4                 12               25      0.3452565
##   0.01        4                 12               50      0.3000285
##   0.01        4                 12               75      0.2651110
##   0.01        4                 12              100      0.2380057
##   0.01        4                 12              125      0.2171301
##   0.01        4                 12              150      0.2008968
##   0.01        4                 12              175      0.1886113
##   0.01        4                 12              200      0.1789947
##   0.01        4                 12              225      0.1715013
##   0.01        4                 12              250      0.1655294
##   0.01        4                 12              275      0.1606849
##   0.01        4                 12              300      0.1566106
##   0.01        4                 12              325      0.1532262
##   0.01        4                 12              350      0.1503582
##   0.01        4                 12              375      0.1479647
##   0.01        8                  8               25      0.3383925
##   0.01        8                  8               50      0.2890479
##   0.01        8                  8               75      0.2518059
##   0.01        8                  8              100      0.2235878
##   0.01        8                  8              125      0.2024777
##   0.01        8                  8              150      0.1865917
##   0.01        8                  8              175      0.1745859
##   0.01        8                  8              200      0.1654353
##   0.01        8                  8              225      0.1585343
##   0.01        8                  8              250      0.1531631
##   0.01        8                  8              275      0.1489450
##   0.01        8                  8              300      0.1455303
##   0.01        8                  8              325      0.1427796
##   0.01        8                  8              350      0.1405103
##   0.01        8                  8              375      0.1386255
##   0.01        8                 12               25      0.3383568
##   0.01        8                 12               50      0.2889004
##   0.01        8                 12               75      0.2514139
##   0.01        8                 12              100      0.2233012
##   0.01        8                 12              125      0.2022465
##   0.01        8                 12              150      0.1863558
##   0.01        8                 12              175      0.1745315
##   0.01        8                 12              200      0.1655396
##   0.01        8                 12              225      0.1587942
##   0.01        8                 12              250      0.1534524
##   0.01        8                 12              275      0.1493118
##   0.01        8                 12              300      0.1459179
##   0.01        8                 12              325      0.1431393
##   0.01        8                 12              350      0.1409278
##   0.01        8                 12              375      0.1391120
##   0.01       12                  8               25      0.3358043
##   0.01       12                  8               50      0.2842559
##   0.01       12                  8               75      0.2455754
##   0.01       12                  8              100      0.2167546
##   0.01       12                  8              125      0.1954135
##   0.01       12                  8              150      0.1797184
##   0.01       12                  8              175      0.1681025
##   0.01       12                  8              200      0.1594269
##   0.01       12                  8              225      0.1530014
##   0.01       12                  8              250      0.1481225
##   0.01       12                  8              275      0.1442958
##   0.01       12                  8              300      0.1412861
##   0.01       12                  8              325      0.1389556
##   0.01       12                  8              350      0.1371109
##   0.01       12                  8              375      0.1355825
##   0.01       12                 12               25      0.3357482
##   0.01       12                 12               50      0.2843208
##   0.01       12                 12               75      0.2456331
##   0.01       12                 12              100      0.2168269
##   0.01       12                 12              125      0.1954587
##   0.01       12                 12              150      0.1797930
##   0.01       12                 12              175      0.1681499
##   0.01       12                 12              200      0.1596977
##   0.01       12                 12              225      0.1533576
##   0.01       12                 12              250      0.1485054
##   0.01       12                 12              275      0.1446811
##   0.01       12                 12              300      0.1416870
##   0.01       12                 12              325      0.1392625
##   0.01       12                 12              350      0.1373953
##   0.01       12                 12              375      0.1359256
##   0.10        4                  8               25      0.1647078
##   0.10        4                  8               50      0.1415839
##   0.10        4                  8               75      0.1365074
##   0.10        4                  8              100      0.1339318
##   0.10        4                  8              125      0.1330176
##   0.10        4                  8              150      0.1329476
##   0.10        4                  8              175      0.1326003
##   0.10        4                  8              200      0.1326350
##   0.10        4                  8              225      0.1326345
##   0.10        4                  8              250      0.1325749
##   0.10        4                  8              275      0.1328210
##   0.10        4                  8              300      0.1326979
##   0.10        4                  8              325      0.1327295
##   0.10        4                  8              350      0.1328382
##   0.10        4                  8              375      0.1328792
##   0.10        4                 12               25      0.1653745
##   0.10        4                 12               50      0.1421282
##   0.10        4                 12               75      0.1370290
##   0.10        4                 12              100      0.1351128
##   0.10        4                 12              125      0.1344596
##   0.10        4                 12              150      0.1337297
##   0.10        4                 12              175      0.1336985
##   0.10        4                 12              200      0.1332621
##   0.10        4                 12              225      0.1331258
##   0.10        4                 12              250      0.1332613
##   0.10        4                 12              275      0.1333579
##   0.10        4                 12              300      0.1333219
##   0.10        4                 12              325      0.1338952
##   0.10        4                 12              350      0.1340763
##   0.10        4                 12              375      0.1343410
##   0.10        8                  8               25      0.1541092
##   0.10        8                  8               50      0.1371983
##   0.10        8                  8               75      0.1343286
##   0.10        8                  8              100      0.1336071
##   0.10        8                  8              125      0.1335343
##   0.10        8                  8              150      0.1337543
##   0.10        8                  8              175      0.1342308
##   0.10        8                  8              200      0.1347300
##   0.10        8                  8              225      0.1349206
##   0.10        8                  8              250      0.1353901
##   0.10        8                  8              275      0.1355738
##   0.10        8                  8              300      0.1356407
##   0.10        8                  8              325      0.1355824
##   0.10        8                  8              350      0.1355159
##   0.10        8                  8              375      0.1356715
##   0.10        8                 12               25      0.1544733
##   0.10        8                 12               50      0.1370520
##   0.10        8                 12               75      0.1339722
##   0.10        8                 12              100      0.1333396
##   0.10        8                 12              125      0.1331105
##   0.10        8                 12              150      0.1336505
##   0.10        8                 12              175      0.1342023
##   0.10        8                 12              200      0.1340503
##   0.10        8                 12              225      0.1343830
##   0.10        8                 12              250      0.1346633
##   0.10        8                 12              275      0.1349855
##   0.10        8                 12              300      0.1352962
##   0.10        8                 12              325      0.1355014
##   0.10        8                 12              350      0.1355756
##   0.10        8                 12              375      0.1356865
##   0.10       12                  8               25      0.1504445
##   0.10       12                  8               50      0.1360067
##   0.10       12                  8               75      0.1341026
##   0.10       12                  8              100      0.1336884
##   0.10       12                  8              125      0.1338430
##   0.10       12                  8              150      0.1340270
##   0.10       12                  8              175      0.1346664
##   0.10       12                  8              200      0.1347403
##   0.10       12                  8              225      0.1350925
##   0.10       12                  8              250      0.1354081
##   0.10       12                  8              275      0.1357271
##   0.10       12                  8              300      0.1360114
##   0.10       12                  8              325      0.1361706
##   0.10       12                  8              350      0.1361946
##   0.10       12                  8              375      0.1363212
##   0.10       12                 12               25      0.1496196
##   0.10       12                 12               50      0.1346752
##   0.10       12                 12               75      0.1324240
##   0.10       12                 12              100      0.1323950
##   0.10       12                 12              125      0.1327764
##   0.10       12                 12              150      0.1328056
##   0.10       12                 12              175      0.1333388
##   0.10       12                 12              200      0.1335975
##   0.10       12                 12              225      0.1342146
##   0.10       12                 12              250      0.1345748
##   0.10       12                 12              275      0.1347084
##   0.10       12                 12              300      0.1351716
##   0.10       12                 12              325      0.1353629
##   0.10       12                 12              350      0.1355215
##   0.10       12                 12              375      0.1355931
##   0.30        4                  8               25      0.1499625
##   0.30        4                  8               50      0.1466066
##   0.30        4                  8               75      0.1472346
##   0.30        4                  8              100      0.1478549
##   0.30        4                  8              125      0.1487538
##   0.30        4                  8              150      0.1489864
##   0.30        4                  8              175      0.1490860
##   0.30        4                  8              200      0.1493763
##   0.30        4                  8              225      0.1496590
##   0.30        4                  8              250      0.1501495
##   0.30        4                  8              275      0.1504262
##   0.30        4                  8              300      0.1505474
##   0.30        4                  8              325      0.1507490
##   0.30        4                  8              350      0.1509365
##   0.30        4                  8              375      0.1510659
##   0.30        4                 12               25      0.1461227
##   0.30        4                 12               50      0.1419950
##   0.30        4                 12               75      0.1424681
##   0.30        4                 12              100      0.1434316
##   0.30        4                 12              125      0.1451267
##   0.30        4                 12              150      0.1456189
##   0.30        4                 12              175      0.1469991
##   0.30        4                 12              200      0.1469274
##   0.30        4                 12              225      0.1473536
##   0.30        4                 12              250      0.1473023
##   0.30        4                 12              275      0.1477565
##   0.30        4                 12              300      0.1480427
##   0.30        4                 12              325      0.1478629
##   0.30        4                 12              350      0.1478680
##   0.30        4                 12              375      0.1484674
##   0.30        8                  8               25      0.1435372
##   0.30        8                  8               50      0.1437492
##   0.30        8                  8               75      0.1440860
##   0.30        8                  8              100      0.1448650
##   0.30        8                  8              125      0.1459724
##   0.30        8                  8              150      0.1469988
##   0.30        8                  8              175      0.1475474
##   0.30        8                  8              200      0.1480721
##   0.30        8                  8              225      0.1481354
##   0.30        8                  8              250      0.1482817
##   0.30        8                  8              275      0.1482142
##   0.30        8                  8              300      0.1484933
##   0.30        8                  8              325      0.1485205
##   0.30        8                  8              350      0.1486360
##   0.30        8                  8              375      0.1486513
##   0.30        8                 12               25      0.1440967
##   0.30        8                 12               50      0.1447820
##   0.30        8                 12               75      0.1463169
##   0.30        8                 12              100      0.1484687
##   0.30        8                 12              125      0.1491975
##   0.30        8                 12              150      0.1499571
##   0.30        8                 12              175      0.1504403
##   0.30        8                 12              200      0.1508033
##   0.30        8                 12              225      0.1513026
##   0.30        8                 12              250      0.1513370
##   0.30        8                 12              275      0.1517777
##   0.30        8                 12              300      0.1518305
##   0.30        8                 12              325      0.1520340
##   0.30        8                 12              350      0.1522572
##   0.30        8                 12              375      0.1520975
##   0.30       12                  8               25      0.1456215
##   0.30       12                  8               50      0.1487357
##   0.30       12                  8               75      0.1492859
##   0.30       12                  8              100      0.1505353
##   0.30       12                  8              125      0.1513679
##   0.30       12                  8              150      0.1518489
##   0.30       12                  8              175      0.1523013
##   0.30       12                  8              200      0.1526899
##   0.30       12                  8              225      0.1528624
##   0.30       12                  8              250      0.1529572
##   0.30       12                  8              275      0.1530758
##   0.30       12                  8              300      0.1531675
##   0.30       12                  8              325      0.1531478
##   0.30       12                  8              350      0.1532067
##   0.30       12                  8              375      0.1532409
##   0.30       12                 12               25      0.1430648
##   0.30       12                 12               50      0.1453452
##   0.30       12                 12               75      0.1470284
##   0.30       12                 12              100      0.1484886
##   0.30       12                 12              125      0.1494353
##   0.30       12                 12              150      0.1500024
##   0.30       12                 12              175      0.1505411
##   0.30       12                 12              200      0.1506628
##   0.30       12                 12              225      0.1509640
##   0.30       12                 12              250      0.1511826
##   0.30       12                 12              275      0.1513079
##   0.30       12                 12              300      0.1512567
##   0.30       12                 12              325      0.1513076
##   0.30       12                 12              350      0.1513967
##   0.30       12                 12              375      0.1513818
##   Rsquared   MAE       
##   0.7651236  0.26496379
##   0.7891393  0.22634306
##   0.8077681  0.19638353
##   0.8198792  0.17337556
##   0.8293840  0.15605108
##   0.8366792  0.14297938
##   0.8429468  0.13302262
##   0.8482420  0.12557451
##   0.8531128  0.11989685
##   0.8574408  0.11554898
##   0.8613746  0.11213464
##   0.8651927  0.10912711
##   0.8686189  0.10652676
##   0.8715764  0.10440642
##   0.8742643  0.10258821
##   0.7646892  0.26491425
##   0.7910511  0.22615275
##   0.8083555  0.19622317
##   0.8210893  0.17320072
##   0.8298947  0.15579669
##   0.8371389  0.14258130
##   0.8427407  0.13275036
##   0.8480856  0.12521334
##   0.8527090  0.11960034
##   0.8569405  0.11520308
##   0.8608683  0.11181527
##   0.8646511  0.10886506
##   0.8680459  0.10639588
##   0.8711556  0.10433602
##   0.8738486  0.10257569
##   0.8112545  0.26001086
##   0.8228775  0.21858427
##   0.8325701  0.18703456
##   0.8419587  0.16323815
##   0.8495089  0.14553119
##   0.8559770  0.13241907
##   0.8613486  0.12256528
##   0.8660967  0.11551555
##   0.8701961  0.11028930
##   0.8739366  0.10623825
##   0.8772821  0.10298785
##   0.8803209  0.10043170
##   0.8830039  0.09832344
##   0.8854072  0.09659676
##   0.8874826  0.09511183
##   0.8107586  0.25989346
##   0.8229724  0.21833574
##   0.8338312  0.18676280
##   0.8422035  0.16304087
##   0.8491188  0.14517757
##   0.8552044  0.13185798
##   0.8601055  0.12215071
##   0.8647701  0.11510808
##   0.8687459  0.11012001
##   0.8725922  0.10609953
##   0.8758975  0.10300975
##   0.8790083  0.10047207
##   0.8818393  0.09836949
##   0.8842991  0.09668979
##   0.8863492  0.09522261
##   0.8316023  0.25822781
##   0.8412312  0.21532397
##   0.8477613  0.18252887
##   0.8541459  0.15816087
##   0.8603970  0.14029499
##   0.8654370  0.12712268
##   0.8699929  0.11769954
##   0.8740928  0.11087037
##   0.8775016  0.10583600
##   0.8805727  0.10203730
##   0.8834661  0.09898276
##   0.8859839  0.09656375
##   0.8881348  0.09474326
##   0.8899528  0.09323810
##   0.8916509  0.09198916
##   0.8293601  0.25818848
##   0.8394855  0.21520220
##   0.8467363  0.18261334
##   0.8533371  0.15818573
##   0.8594848  0.14026516
##   0.8643120  0.12722940
##   0.8688887  0.11765785
##   0.8727155  0.11093749
##   0.8760776  0.10607641
##   0.8793832  0.10238702
##   0.8823570  0.09944839
##   0.8850347  0.09715730
##   0.8873817  0.09522601
##   0.8893473  0.09367306
##   0.8909573  0.09238747
##   0.8539561  0.11596949
##   0.8802987  0.09845143
##   0.8879782  0.09414679
##   0.8919759  0.09173839
##   0.8933833  0.09052768
##   0.8935322  0.08978893
##   0.8939934  0.08938416
##   0.8938438  0.08919291
##   0.8938225  0.08937104
##   0.8940942  0.08924915
##   0.8935694  0.08944192
##   0.8936951  0.08920117
##   0.8936992  0.08930539
##   0.8934746  0.08934966
##   0.8935827  0.08926171
##   0.8531775  0.11582919
##   0.8794895  0.09886354
##   0.8874542  0.09421534
##   0.8901011  0.09201085
##   0.8911721  0.09113868
##   0.8923065  0.09042671
##   0.8924118  0.09039510
##   0.8929009  0.09021734
##   0.8930152  0.08999923
##   0.8929315  0.08981287
##   0.8927955  0.08979754
##   0.8928506  0.08955739
##   0.8919495  0.08997533
##   0.8916451  0.09008403
##   0.8911953  0.09048664
##   0.8681883  0.10746588
##   0.8875780  0.09418395
##   0.8918886  0.09122672
##   0.8928251  0.09007975
##   0.8927829  0.08959682
##   0.8924272  0.08957312
##   0.8916077  0.08991647
##   0.8908573  0.09019672
##   0.8905876  0.09028438
##   0.8898347  0.09060080
##   0.8894747  0.09066832
##   0.8893575  0.09070921
##   0.8894189  0.09078325
##   0.8895187  0.09082851
##   0.8892139  0.09089297
##   0.8671288  0.10764718
##   0.8871647  0.09423486
##   0.8918706  0.09122199
##   0.8925864  0.09020750
##   0.8931491  0.09010055
##   0.8922539  0.09028074
##   0.8913171  0.09055964
##   0.8915660  0.09055722
##   0.8910545  0.09076030
##   0.8906069  0.09103542
##   0.8900006  0.09126831
##   0.8893882  0.09164174
##   0.8891715  0.09192652
##   0.8888672  0.09207022
##   0.8886800  0.09204297
##   0.8725565  0.10460531
##   0.8887067  0.09254720
##   0.8915615  0.09080128
##   0.8920313  0.09009569
##   0.8917789  0.09005162
##   0.8913984  0.09009747
##   0.8903708  0.09045912
##   0.8902406  0.09032346
##   0.8897069  0.09060740
##   0.8891738  0.09081561
##   0.8886063  0.09107393
##   0.8881438  0.09138539
##   0.8879455  0.09145006
##   0.8879677  0.09152190
##   0.8877395  0.09166141
##   0.8744089  0.10358955
##   0.8916202  0.09225516
##   0.8948435  0.08998163
##   0.8948205  0.08979576
##   0.8940652  0.09020299
##   0.8939546  0.09023954
##   0.8930319  0.09061715
##   0.8926182  0.09087683
##   0.8915625  0.09128114
##   0.8908505  0.09140868
##   0.8906193  0.09153363
##   0.8898956  0.09197366
##   0.8895420  0.09224637
##   0.8893675  0.09239485
##   0.8892256  0.09249733
##   0.8652984  0.10572265
##   0.8713191  0.10222863
##   0.8706982  0.10213132
##   0.8692157  0.10199746
##   0.8678188  0.10278420
##   0.8679518  0.10299456
##   0.8676171  0.10260624
##   0.8671427  0.10267012
##   0.8666055  0.10311525
##   0.8656796  0.10329396
##   0.8653120  0.10350163
##   0.8651273  0.10321109
##   0.8648691  0.10368084
##   0.8646346  0.10372356
##   0.8643651  0.10393036
##   0.8715861  0.10243048
##   0.8791450  0.09860509
##   0.8785209  0.09872562
##   0.8767319  0.09913898
##   0.8744342  0.10043017
##   0.8733001  0.10092308
##   0.8713639  0.10200649
##   0.8714327  0.10222631
##   0.8707584  0.10247710
##   0.8706864  0.10210316
##   0.8697826  0.10257550
##   0.8693627  0.10278358
##   0.8697981  0.10273765
##   0.8698046  0.10266907
##   0.8689764  0.10282385
##   0.8767436  0.09893627
##   0.8769985  0.09868884
##   0.8763105  0.09914493
##   0.8752737  0.09995946
##   0.8736924  0.10072030
##   0.8718282  0.10131972
##   0.8709373  0.10165843
##   0.8702875  0.10223294
##   0.8700902  0.10255108
##   0.8699615  0.10261243
##   0.8701951  0.10269517
##   0.8696560  0.10291570
##   0.8695704  0.10298562
##   0.8694069  0.10300340
##   0.8693245  0.10299288
##   0.8752338  0.10069336
##   0.8738378  0.10149873
##   0.8714954  0.10228740
##   0.8678792  0.10439422
##   0.8667180  0.10518297
##   0.8652488  0.10533575
##   0.8643525  0.10570313
##   0.8636060  0.10591855
##   0.8627930  0.10627864
##   0.8626587  0.10632338
##   0.8619915  0.10673910
##   0.8618701  0.10682798
##   0.8615090  0.10694990
##   0.8610791  0.10715846
##   0.8613790  0.10699383
##   0.8740978  0.10254809
##   0.8687351  0.10486698
##   0.8678133  0.10551638
##   0.8660464  0.10649301
##   0.8647510  0.10694547
##   0.8639936  0.10762528
##   0.8633386  0.10812369
##   0.8626578  0.10841461
##   0.8623389  0.10855931
##   0.8622155  0.10862256
##   0.8620539  0.10865657
##   0.8618754  0.10875689
##   0.8619239  0.10869107
##   0.8618580  0.10873775
##   0.8617939  0.10879089
##   0.8767362  0.10011144
##   0.8728525  0.10224080
##   0.8704188  0.10338459
##   0.8683055  0.10460708
##   0.8666816  0.10520470
##   0.8660167  0.10518072
##   0.8651036  0.10555704
##   0.8649507  0.10575473
##   0.8644655  0.10605181
##   0.8641400  0.10612164
##   0.8639422  0.10623995
##   0.8640334  0.10624049
##   0.8639591  0.10625971
##   0.8637792  0.10630486
##   0.8637919  0.10630135
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were n.trees = 100,
##  interaction.depth = 12, shrinkage = 0.1 and n.minobsinnode = 12.</code></pre>
<p>Agora só falta ajustar o modelo final utilizando a combinação de hiperparâmetros que obteve o melhor desempenho.</p>
<pre class="r"><code>model.gbm.final &lt;- gbm(SalePrice ~ ., data = treino, 
                       distribution = &quot;gaussian&quot;, 
                       n.trees = 100, 
                       shrinkage = 0.1, 
                       interaction.depth = 12, 
                       n.minobsinnode = 12)

rmse.gbm &lt;- RMSE(exp(predict(model.gbm.final,validacao, n.trees = 100)), exp(validacao$SalePrice))
rmse.gbm</code></pre>
<pre><code>## [1] 26431.45</code></pre>
<h3>
Ensemble
</h3>
<p>Para fazer o ensemble eu vou utilizar um modelo linear para estimar os pesos do modelo. Primeiro eu vou criar um outro data frame contendo as previsões de cada modelo estimado até aqui.</p>
<pre class="r"><code>ensemble.data.frame &lt;- data.frame(SalePrice = treino$SalePrice, randomforest = predict(model.rf.final,treino), 
                                  gbm =predict(model.gbm.final,treino, n.trees = 100),
                                  svm = predict(model.svm.final, treino))</code></pre>
<p>Agora ajustar o modelo</p>
<pre class="r"><code>model.lm &lt;- lm(SalePrice~randomforest+gbm+svm+0, data= ensemble.data.frame)
summary(model.lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = SalePrice ~ randomforest + gbm + svm + 0, data = ensemble.data.frame)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.54071 -0.02458  0.00252  0.02851  0.25265 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## randomforest  0.84983    0.04123  20.611  &lt; 2e-16 ***
## gbm           0.24081    0.04040   5.961 3.32e-09 ***
## svm          -0.09075    0.01690  -5.369 9.54e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.05604 on 1165 degrees of freedom
## Multiple R-squared:      1,  Adjusted R-squared:      1 
## F-statistic: 1.794e+07 on 3 and 1165 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>e calcular o RMSE utilizando os pesos estimados.</p>
<pre class="r"><code>rmse.ensemble &lt;- RMSE(0.84983*exp(predict(model.rf.final, validacao))+ 
       0.24081*exp(predict(model.gbm.final, validacao, n.trees = 100)) -
       0.09075*exp(predict(model.svm.final, validacao)), exp(validacao$SalePrice))
rmse.ensemble</code></pre>
<pre><code>## [1] 28650.25</code></pre>
<h2>
Conclusão
</h2>
<p>Eu estimei três modelos e fiz um ensemble entre eles. Na tabela abaixo eu mostro o RMSE calculado em cada cenário.</p>
<pre><code>##                                  RMSE
## Random Forest                29332.11
## Support Vector Machine       32341.42
## Stochastic Gradient Boosting 26431.45
## Ensemble                     28650.25</code></pre>
<p>O stochastic gradient boosting foi o modelo com melhor desempenho, seguido pelo ensemble e pelo Random Forest. Não consegui obter um bom desempenho com o support vector machine. Talvez com um melhor tuning dos parâmetros possa se obter melhor desempenho do método, mas o meu atual poder computacional não permite que eu teste um grid muito grande de valores.</p>

    </div>
    

    

    </div>

      </div>

      
        <footer class="footer">
  <div class="footer__inner">
    
      <a href="/" class="logo" style="text-decoration: none;">
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367"/>
</svg>
</span>
    <span class="logo__text">Jefferson Barbosa</span>
    <span class="logo__cursor"></span>
  
</a>

      <div class="copyright">
        <span>© 2019 Powered by <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a></span>
        <span>Theme created by <a href="https://twitter.com/panr" target="_blank" rel="noopener">panr</a></span>
      </div>
    
  </div>
</footer>

<script src="/assets/main.js"></script>
<script src="/assets/prism.js"></script>


      
    </div>

    
  </body>
</html>
